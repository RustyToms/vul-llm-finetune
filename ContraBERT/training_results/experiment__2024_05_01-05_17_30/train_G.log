05/01/2024 05:17:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at /home/ma-user/saved_models/pretrain_models/ContraBERT_G were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/ma-user/saved_models/pretrain_models/ContraBERT_G and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/01/2024 05:17:34 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='/home/ma-user/data/train.jsonl', output_dir='/home/ma-user/output/experiment__2024_05_01-05_17_30', eval_data_file=None, eval_data_files={'eval': '/home/ma-user/data/valid.jsonl', 'test': '/home/ma-user/data/test.jsonl'}, test_data_file='/home/ma-user/data/test.jsonl', model_type='roberta', model_name_or_path='/home/ma-user/saved_models/pretrain_models/ContraBERT_G', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='/home/ma-user/saved_models/base_model/models--microsoft--codebert-base', block_size=400, do_train=True, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=1.2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=10, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', max_pool=True, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
05/01/2024 05:17:35 - INFO - __main__ -   *** Example ***
05/01/2024 05:17:35 - INFO - __main__ -   idx: 0
05/01/2024 05:17:35 - INFO - __main__ -   label: 0
05/01/2024 05:17:35 - INFO - __main__ -   input_tokens: ['<s>', 'int', '_curl', '_', 'm', 'vs', 'printf', '(', 'char', '*', '_buffer', ',', '_const', '_char', '*', '_format', ',', '_va', '_', 'list', '_ap', '_', 'save', ')', '_{', '_int', '_ret', 'code', ';', '_ret', 'code', '_=', '_vs', 'n', 'printf', '(', 'buffer', ',', '_BU', 'FFER', '_', 'SIZE', ',', '_format', ',', '_ap', '_', 'save', ');', '_buffer', '[', 'BU', 'FFER', '_', 'SIZE', '_-', '_1', ']', '_=', "_'", '\\', '0', "';", '_return', '_ret', 'code', ';', '_}', '</s>']
05/01/2024 05:17:35 - INFO - __main__ -   input_ids: 0 2544 37718 1215 119 15597 49775 1640 24262 3226 21944 6 10759 16224 3226 7390 6 13205 1215 8458 6256 1215 31575 43 25522 6979 5494 20414 131 5494 20414 5457 1954 282 49775 1640 47438 6 17149 45234 1215 49340 6 7390 6 6256 1215 31575 4397 21944 10975 19159 45234 1215 49340 111 112 742 5457 128 37457 288 23500 671 5494 20414 131 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
05/01/2024 05:17:36 - INFO - __main__ -   ***** Running training *****
05/01/2024 05:17:36 - INFO - __main__ -     Num examples = 1410
05/01/2024 05:17:36 - INFO - __main__ -     Num Epochs = 10
05/01/2024 05:17:36 - INFO - __main__ -     Instantaneous batch size per GPU = 32
05/01/2024 05:17:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/01/2024 05:17:36 - INFO - __main__ -     Gradient Accumulation steps = 1
05/01/2024 05:17:36 - INFO - __main__ -     Total optimization steps = 450
Setting up tensorboard writer, metrics saved at /home/ma-user/output/experiment__2024_05_01-05_17_30
CUDA is available.
Number of GPUs available:  1
---- GPU 0 ----
Name: NVIDIA GeForce RTX 3090
Total Memory: 24575.5 MB
Capability: 8.6
----------
local_rank -1
co_cuda False
microsoft/codebert-base False /home/ma-user/saved_models/base_model/models--microsoft--codebert-base
  0%|          | 0/45 [00:00<?, ?it/s]epoch 0 loss 0.71631:   0%|          | 0/45 [00:01<?, ?it/s]epoch 0 loss 0.71631:   2%|▏         | 1/45 [00:01<00:52,  1.18s/it]epoch 0 loss 0.75234:   2%|▏         | 1/45 [00:01<00:52,  1.18s/it]epoch 0 loss 0.75234:   4%|▍         | 2/45 [00:01<00:34,  1.25it/s]epoch 0 loss 0.74825:   4%|▍         | 2/45 [00:02<00:34,  1.25it/s]epoch 0 loss 0.74825:   7%|▋         | 3/45 [00:02<00:28,  1.47it/s]epoch 0 loss 0.73053:   7%|▋         | 3/45 [00:02<00:28,  1.47it/s]epoch 0 loss 0.73053:   9%|▉         | 4/45 [00:02<00:24,  1.65it/s]epoch 0 loss 0.73471:   9%|▉         | 4/45 [00:03<00:24,  1.65it/s]epoch 0 loss 0.73471:  11%|█         | 5/45 [00:03<00:22,  1.75it/s]epoch 0 loss 0.72747:  11%|█         | 5/45 [00:03<00:22,  1.75it/s]epoch 0 loss 0.72747:  13%|█▎        | 6/45 [00:03<00:21,  1.79it/s]epoch 0 loss 0.72991:  13%|█▎        | 6/45 [00:04<00:21,  1.79it/s]epoch 0 loss 0.72991:  16%|█▌        | 7/45 [00:04<00:21,  1.79it/s]epoch 0 loss 0.72386:  16%|█▌        | 7/45 [00:04<00:21,  1.79it/s]epoch 0 loss 0.72386:  18%|█▊        | 8/45 [00:04<00:20,  1.82it/s]epoch 0 loss 0.72703:  18%|█▊        | 8/45 [00:05<00:20,  1.82it/s]epoch 0 loss 0.72703:  20%|██        | 9/45 [00:05<00:19,  1.87it/s]epoch 0 loss 0.72507:  20%|██        | 9/45 [00:05<00:19,  1.87it/s]epoch 0 loss 0.72507:  22%|██▏       | 10/45 [00:05<00:18,  1.88it/s]epoch 0 loss 0.72885:  22%|██▏       | 10/45 [00:06<00:18,  1.88it/s]epoch 0 loss 0.72885:  24%|██▍       | 11/45 [00:06<00:17,  1.92it/s]epoch 0 loss 0.72929:  24%|██▍       | 11/45 [00:06<00:17,  1.92it/s]epoch 0 loss 0.72929:  27%|██▋       | 12/45 [00:06<00:17,  1.87it/s]epoch 0 loss 0.7311:  27%|██▋       | 12/45 [00:07<00:17,  1.87it/s] epoch 0 loss 0.7311:  29%|██▉       | 13/45 [00:07<00:16,  1.91it/s]epoch 0 loss 0.72853:  29%|██▉       | 13/45 [00:07<00:16,  1.91it/s]epoch 0 loss 0.72853:  31%|███       | 14/45 [00:08<00:16,  1.89it/s]epoch 0 loss 0.72448:  31%|███       | 14/45 [00:08<00:16,  1.89it/s]epoch 0 loss 0.72448:  33%|███▎      | 15/45 [00:08<00:15,  1.92it/s]epoch 0 loss 0.72687:  33%|███▎      | 15/45 [00:09<00:15,  1.92it/s]epoch 0 loss 0.72687:  36%|███▌      | 16/45 [00:09<00:15,  1.89it/s]epoch 0 loss 0.72609:  36%|███▌      | 16/45 [00:09<00:15,  1.89it/s]epoch 0 loss 0.72609:  38%|███▊      | 17/45 [00:09<00:14,  1.88it/s]epoch 0 loss 0.72546:  38%|███▊      | 17/45 [00:10<00:14,  1.88it/s]epoch 0 loss 0.72546:  40%|████      | 18/45 [00:10<00:14,  1.88it/s]epoch 0 loss 0.72534:  40%|████      | 18/45 [00:10<00:14,  1.88it/s]epoch 0 loss 0.72534:  42%|████▏     | 19/45 [00:10<00:13,  1.89it/s]epoch 0 loss 0.72363:  42%|████▏     | 19/45 [00:11<00:13,  1.89it/s]epoch 0 loss 0.72363:  44%|████▍     | 20/45 [00:11<00:13,  1.92it/s]epoch 0 loss 0.7228:  44%|████▍     | 20/45 [00:11<00:13,  1.92it/s] epoch 0 loss 0.7228:  47%|████▋     | 21/45 [00:11<00:12,  1.94it/s]epoch 0 loss 0.72249:  47%|████▋     | 21/45 [00:12<00:12,  1.94it/s]epoch 0 loss 0.72249:  49%|████▉     | 22/45 [00:12<00:11,  1.94it/s]epoch 0 loss 0.72149:  49%|████▉     | 22/45 [00:12<00:11,  1.94it/s]epoch 0 loss 0.72149:  51%|█████     | 23/45 [00:12<00:11,  1.95it/s]epoch 0 loss 0.7213:  51%|█████     | 23/45 [00:13<00:11,  1.95it/s] epoch 0 loss 0.7213:  53%|█████▎    | 24/45 [00:13<00:11,  1.89it/s]epoch 0 loss 0.72075:  53%|█████▎    | 24/45 [00:13<00:11,  1.89it/s]epoch 0 loss 0.72075:  56%|█████▌    | 25/45 [00:13<00:10,  1.90it/s]epoch 0 loss 0.7199:  56%|█████▌    | 25/45 [00:14<00:10,  1.90it/s] epoch 0 loss 0.7199:  58%|█████▊    | 26/45 [00:14<00:09,  1.93it/s]epoch 0 loss 0.71951:  58%|█████▊    | 26/45 [00:14<00:09,  1.93it/s]epoch 0 loss 0.71951:  60%|██████    | 27/45 [00:14<00:09,  1.88it/s]epoch 0 loss 0.71834:  60%|██████    | 27/45 [00:15<00:09,  1.88it/s]epoch 0 loss 0.71834:  62%|██████▏   | 28/45 [00:15<00:09,  1.87it/s]epoch 0 loss 0.71788:  62%|██████▏   | 28/45 [00:15<00:09,  1.87it/s]epoch 0 loss 0.71788:  64%|██████▍   | 29/45 [00:15<00:08,  1.91it/s]epoch 0 loss 0.71696:  64%|██████▍   | 29/45 [00:16<00:08,  1.91it/s]epoch 0 loss 0.71696:  67%|██████▋   | 30/45 [00:16<00:07,  1.88it/s]epoch 0 loss 0.71634:  67%|██████▋   | 30/45 [00:16<00:07,  1.88it/s]epoch 0 loss 0.71634:  69%|██████▉   | 31/45 [00:16<00:07,  1.88it/s]epoch 0 loss 0.71605:  69%|██████▉   | 31/45 [00:17<00:07,  1.88it/s]epoch 0 loss 0.71605:  71%|███████   | 32/45 [00:17<00:06,  1.87it/s]epoch 0 loss 0.71511:  71%|███████   | 32/45 [00:17<00:06,  1.87it/s]epoch 0 loss 0.71511:  73%|███████▎  | 33/45 [00:18<00:06,  1.86it/s]epoch 0 loss 0.71495:  73%|███████▎  | 33/45 [00:18<00:06,  1.86it/s]epoch 0 loss 0.71495:  76%|███████▌  | 34/45 [00:18<00:05,  1.90it/s]epoch 0 loss 0.7141:  76%|███████▌  | 34/45 [00:19<00:05,  1.90it/s] epoch 0 loss 0.7141:  78%|███████▊  | 35/45 [00:19<00:05,  1.92it/s]epoch 0 loss 0.7132:  78%|███████▊  | 35/45 [00:19<00:05,  1.92it/s]epoch 0 loss 0.7132:  80%|████████  | 36/45 [00:19<00:04,  1.93it/s]epoch 0 loss 0.71289:  80%|████████  | 36/45 [00:20<00:04,  1.93it/s]epoch 0 loss 0.71289:  82%|████████▏ | 37/45 [00:20<00:04,  1.91it/s]epoch 0 loss 0.71255:  82%|████████▏ | 37/45 [00:20<00:04,  1.91it/s]epoch 0 loss 0.71255:  84%|████████▍ | 38/45 [00:20<00:03,  1.93it/s]epoch 0 loss 0.71138:  84%|████████▍ | 38/45 [00:21<00:03,  1.93it/s]epoch 0 loss 0.71138:  87%|████████▋ | 39/45 [00:21<00:03,  1.88it/s]epoch 0 loss 0.71129:  87%|████████▋ | 39/45 [00:21<00:03,  1.88it/s]epoch 0 loss 0.71129:  89%|████████▉ | 40/45 [00:21<00:02,  1.85it/s]epoch 0 loss 0.71053:  89%|████████▉ | 40/45 [00:22<00:02,  1.85it/s]epoch 0 loss 0.71053:  91%|█████████ | 41/45 [00:22<00:02,  1.88it/s]epoch 0 loss 0.71005:  91%|█████████ | 41/45 [00:22<00:02,  1.88it/s]epoch 0 loss 0.71005:  93%|█████████▎| 42/45 [00:22<00:01,  1.91it/s]epoch 0 loss 0.70934:  93%|█████████▎| 42/45 [00:23<00:01,  1.91it/s]epoch 0 loss 0.70934:  96%|█████████▌| 43/45 [00:23<00:01,  1.88it/s]epoch 0 loss 0.70866:  96%|█████████▌| 43/45 [00:23<00:01,  1.88it/s]epoch 0 loss 0.70866:  98%|█████████▊| 44/45 [00:23<00:00,  1.91it/s]epoch 0 loss 0.71141:  98%|█████████▊| 44/45 [00:23<00:00,  1.91it/s]epoch 0 loss 0.71141:  98%|█████████▊| 44/45 [00:23<00:00,  1.84it/s]
eval_data_files: {'eval': '/home/ma-user/data/valid.jsonl', 'test': '/home/ma-user/data/test.jsonl'}
Traceback (most recent call last):
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 644, in <module>
    main()
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 615, in main
    train(args, train_dataset, model, tokenizer, writer)
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 250, in train
    results = evaluate(args, model, tokenizer, eval_when_training=True)
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 281, in evaluate
    eval_datasets[name] = TextDataset(tokenizer, args, dataset_name)
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 114, in __init__
    self.examples.append(convert_examples_to_features(js, tokenizer, args, isdevign))
  File "/home/ma-user/defect_detection/vulnerability_detection.py", line 97, in convert_examples_to_features
    return InputFeatures(source_tokens, source_ids, js['idx'], js['target'])
KeyError: 'idx'
